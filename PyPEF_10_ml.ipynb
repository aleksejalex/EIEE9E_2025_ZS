{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleksejalex/EIEE9E_2025_ZS/blob/main/PyPEF_10_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRbDDh0Uf25z"
      },
      "source": [
        "# PyPEF, lecture 10. Intro to ML.\n",
        "\n",
        "Prepared by: Aleksej Gaj ( pythonforstudents24@gmail.com )\n",
        "\n",
        "üîó Course website: [https://aleksejalex.4fan.cz/pef_python/](https://aleksejalex.4fan.cz/pef_python/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRlswSHQ2ZGe"
      },
      "source": [
        "## Motivation\n",
        "We would like computers to perform complicated tasks (medical diagnosis, stocks prediction, ...).\n",
        "\n",
        "2 artificial intelligence approaches are distinguished:\n",
        " - **knowledge-based**üìöü¶â: a computer program whose logic encodes a large number of properties of the world, usually developed by a team of experts over many years. a.k.a. *knowledge-based* approach\n",
        " - **machine learning**üîÆüë®üèª‚Äçüíª: extract information from past observations (data) and extrapolate to make predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRoCPFdVjWnR"
      },
      "source": [
        "Plan for this lecture:\n",
        " - intro to machine learning\n",
        " - perceptron: regression task seen differently\n",
        " - classifiers: split the data into groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njErkJvA2ZGg"
      },
      "source": [
        "## Machine learning\n",
        " = field of study in AI, main aim: development of statistical **algorithms that can learn from data and generalize to unseen data**\n",
        " - *\"Field of study that gives computers the ability to learn without being explicitly programmed.\"* - Arthur Samuel, 1959\n",
        "\n",
        "Types of ML:\n",
        " - **supervised**: learns connection between input ($x$) and output ($y$); the algortihm learns from given examples of \"correct answers\"\n",
        "   - examples: spam filtering, machine translation, speech recognition, online advertising, ...\n",
        " - **unsupervised**: learns patterns from input data without explicit supervision or labeled outcomes; seeks to find hidden structure or relationships in the data without guidance from a labeled target variable\n",
        "   - examples: clustering customers based on purchasing behavior, reducing the number of features in a dataset while preserving its important information, discovering interesting associations or relationships between variables in large datasets(market basket analysis to identify frequently co-occurring items in transactions), ...\n",
        " - **reinforcement**: agent learns to make decisions by interacting with an environment. The agent learns from feedback in the form of rewards or penalties received for its actions, with the goal of maximizing cumulative rewards over time\n",
        "   - examples: robotics, game playing, autonomous vehicles, recommendation systems, ...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M00WQDIH2ZGg"
      },
      "source": [
        "*Example from (your) life:*\n",
        " - when you learned in primary school how animals are classified, it was *supervised learning*: you were given both animal names and the correct classification (Amphibia, Mammalia, ...) and you understood the connecting principle (or memorised it)\n",
        "\n",
        " - when a child learns do differentiate colors, it's an *unsupervised learning*: it 'automatically' sees that marbles are of different colors, and is able to split them into red ones and blue ones. Noone explains to him what actually is red colour and how it's different from a blue one\n",
        "\n",
        " - when you were in driving school, it was *reinforcement learning*: the instructor told you after every maneuveur if you got it right or wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRZ94WnF2ZGh"
      },
      "source": [
        "[structure pie chart](https://chart-studio.plotly.com/create/?fid=SolClover:40#/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ51YhZ52ZGi"
      },
      "source": [
        "<img src=\"https://lh7-us.googleusercontent.com/xOCj9XQMPcLFix1tN828tExM8ElqNebmXiGJcl7lzCwQsk1M6C6e17zsgrq1el5VnpW6wtxQeGtOG-EETQeRFvnkNMClwD0rDQ7Uit0z8Xq73ZZWd4XDP4W0bHhDlY-ZiFIe8oHsvREZcG4q03K7crM\" alt=\"banner\" width=\"700\" align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M64I_kAa2ZGi"
      },
      "source": [
        "### Two basic tasks/approaches:\n",
        "\n",
        " - **regression** = fitting a \"shape\" to data as close as possible\n",
        "\n",
        "<img src=\"https://higherlogicdownload.s3.amazonaws.com/IMWUC/UploadedImages/92757287-d116-4157-b004-c2a0aba1b048/linear-regression-in-machine-learning.png\" alt=\"banner\" width=\"300\" align=\"center\"><br>\n",
        " *Examples:* predict the age of a cat based on its weight, predict price of a house based on its size and location, predict failure of some kind of machinery, ...\n",
        "\n",
        " - **classification** = separating data into classes (response is represented by discrete values)\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:980/1*wm5m2Wd0gMXAkztc7vP7yA.png\" alt=\"banner\" width=\"260\" align=\"center\"><br>\n",
        "*Examples:* predict whether the creature is mammal or not, predict whether there is a man or woman on the picture, predict what scene is on the video, ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTwY1hrd2ZGj"
      },
      "source": [
        "Before we proceed further, we will need some libraries we are already familiar with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s58i0mVK2ZGj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfUKG-1o2ZGk"
      },
      "source": [
        "## scikit-learn\n",
        "<a href=\"https://scikit-learn.org/stable/index.html\"><img src=\"https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" alt=\"banner\" width=\"300\" align=\"right\"></a>\n",
        " = library offering algorithms for classification, regression, clustering, etc.\n",
        " - user friendly, doesn't require deep knowledge\n",
        " - compatible with NumPy, SciPy, matplotlib and Pandas\n",
        " - has very nice documentation [here](https://scikit-learn.org/stable/index.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WXmnrMqcZ9X"
      },
      "outputs": [],
      "source": [
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa8wD4232ZGl"
      },
      "source": [
        "Before we get to ML, let's get familiar with some functions we will use:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V48L0j822ZGl"
      },
      "source": [
        " - **generate random data in specific way** (so you can easily train your classifying skills)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUHPs7bc2ZGl"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Generate random data using make_blobs\n",
        "num_samples = 15\n",
        "x, y = make_blobs(n_samples=num_samples, centers=2, cluster_std=1.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P4IZvT22ZGl"
      },
      "source": [
        "function output are pairs, each pair is: $(\\vec{x}, y)$, where:\n",
        " - $y \\in \\lbrace 0, 1, 2, ... N \\rbrace $, where $N$ is number of classes ... index, to which class the observation belongs\n",
        " - $\\vec{x} = (x_1, x_2, ..., x_a)$, where $x_j$ is a value of $j$-th feature ... vector of features (example: if our observation are customers, their features can be: age, gender, amount of spent money, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eucyhrzm2ZGl"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgucMrbb2ZGm"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMWNZRij2ZGn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(4, 3))\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y, cmap='viridis', marker='o')\n",
        "plt.title('Visualization of Blobs')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAPIjMzQ2ZGn"
      },
      "source": [
        "You can see there basically 2 gaussian hats (~ 15 samples from 2 gaussian hats)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl_EbtCb2ZGn"
      },
      "source": [
        "<a href=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmedia.geeksforgeeks.org%2Fwp-content%2Fcdn-uploads%2F20190523171258%2Foverfitting_2.png&f=1&nofb=1&ipt=042ce96dbef98d7aa8ba54ca1e90ce3d0132347982c54dd1ca7580e6da8411e3&ipo=images\"><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmedia.geeksforgeeks.org%2Fwp-content%2Fcdn-uploads%2F20190523171258%2Foverfitting_2.png&f=1&nofb=1&ipt=042ce96dbef98d7aa8ba54ca1e90ce3d0132347982c54dd1ca7580e6da8411e3&ipo=images\" alt=\"banner\" width=\"600\" align=\"right\"></a>\n",
        " - function that splits our data (`X, y`) into training dataset and testing dataset.\n",
        "This is needed to evaluate how successful are the predictions of our model on those data that it hasn't seen. (The ability to extrapolate is important.)\n",
        "\n",
        "Also this helps to avoid overfitting: if model learns to much from our data, it starts to remember not only the main pattern, but also things like random noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCUtF3u82ZGn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Hbb82P2ZGo"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLhtSVx92ZGo"
      },
      "outputs": [],
      "source": [
        "print(f\"size of x = {x.shape}\")\n",
        "print(f\"size of X_train = {X_train.shape}\")\n",
        "print(f\"size of X_test = {X_test.shape}\")\n",
        "print(f\"size of y = {y.shape}\")\n",
        "print(f\"size of y_train = {y_train.shape}\")\n",
        "print(f\"size of y_test = {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny6f264g2ZGo"
      },
      "outputs": [],
      "source": [
        "# we don't want to mess with different variables onder same name, so:\n",
        "del(x, y, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTkzkgZD2ZGp"
      },
      "source": [
        "### Perceptron - an \"atom\" of neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZHonlOF2ZGp"
      },
      "source": [
        "<a href=\"https://media.geeksforgeeks.org/wp-content/uploads/20230426162726/Perceptron-1.webp\"><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230426162726/Perceptron-1.webp\" alt=\"banner\" width=\"500\" align=\"right\"></a>\n",
        "\n",
        "recommended reading:\n",
        " - simpliest tutorial on [geeksforgeeks](https://www.geeksforgeeks.org/what-is-perceptron-the-simplest-artificial-neural-network/?ref=header_search)\n",
        " - more details and history on [wikipedia](https://en.wikipedia.org/wiki/Perceptron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cMdRVA82ZGp"
      },
      "source": [
        "**Perceptron** is the simplest neural network consisting of single node. One of it's benefits is simple implementation and relatively high accuracy, but it can be used only when these assumptions are made:\n",
        "1.   data are *linearly separable*\n",
        "2.   classification problem is *binary*\n",
        "\n",
        "Perceptron *finds one hyperplane* (for example in 2D - line) that separates the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JJzWDVP2ZGp"
      },
      "source": [
        "**Decision rule:**\n",
        "to check whether datapoint $x$ lies on positive or negative side of the hyperplane.\n",
        "\n",
        " - mathematically hyperplane is a linear function defined in vector space:\n",
        "\\begin{aligned}\n",
        "f(x) = \\vec{w} \\cdot \\vec{x} + b = w^{t} x + b = \\sum_j w_j \\cdot x_j + b = \\sum_j w_j \\cdot x_j + b \\cdot 1,\n",
        "\\end{aligned}\n",
        "\n",
        "where $x$ is positional vector, $w$ and $b$ are coeficients (also vectors) defining concrete hyperplane out of all existing hyperplanes in defined space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D6iAcmU2ZGp"
      },
      "source": [
        " - to determine \"on which side\" of hyperplane is point $x$ we use activation function - here Heaviside (step) function defined as\n",
        "\\begin{align}\n",
        "        h(x) = \\left\\{\n",
        "        \\begin{array}{cl}\n",
        "        1 & \\text{if } x \\ge 0 \\\\\n",
        "        0 & \\text{otherwise}\n",
        "        \\end{array}\n",
        "        \\right.\n",
        "    \\end{align}\n",
        "\n",
        "\n",
        "* let's note $\\hat{y}(x) := h\\left( w^tx + b \\right)$ (**approximation/prediction**) This formula returns either 1 or 0 depending on which side of hyperplane is located point $x$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBrO2-hh2ZGp"
      },
      "source": [
        "**Update rule:** now we have our result $\\hat{y}$ (computed above) and also real value (part of train data). We want to check whether our result is equal to real value or not. In second case we want to change parameters $w$ and $b$ (rotate/shift the hyperplane) in preferred way. In other words - if our prediction was wrong, we want to update our parameters describing the hyperplane.\n",
        "\n",
        "\\begin{aligned}\n",
        "w = w + \\alpha (<y|x> - <\\hat{y}|x>) = w + \\alpha (y - \\hat{y})x,\n",
        "\\end{aligned}\n",
        "\n",
        "where $\\alpha \\in [0,1]$ is *learning rate*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ZbQ1cN2ZGq"
      },
      "source": [
        "<a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/900px-Perceptron_example.svg.png\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/900px-Perceptron_example.svg.png\" alt=\"banner\" width=\"370\" align=\"right\"></a>\n",
        "\n",
        "*Technically*, update looks like this:\n",
        " - we try to make a prediction\n",
        " - we compute the loss (since we have the real value of `y` and predicted one, we can compute the difference)\n",
        " - we update weights and bias ($w$ and $b$) accordingly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yz4QU6x2ZGq"
      },
      "source": [
        "Perceptron implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLRCzHUu2ZGr"
      },
      "outputs": [],
      "source": [
        "class OurPerceptron:\n",
        "    def __init__(self, num_inputs, learning_rate=0.01):\n",
        "        # Initialize weights and learning rate\n",
        "        self.weights = np.random.rand(num_inputs + 1)  # +1 for bias (b)\n",
        "        self.learning_rate = learning_rate  # controls how much we adjust the weights with each step of training\n",
        "\n",
        "    def linear(self, inputs):\n",
        "        # Compute the linear combination of inputs and weights\n",
        "        return np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
        "\n",
        "    def Heaviside_step_fn(self, z):\n",
        "        # Heaviside step function\n",
        "        return np.heaviside(z, 1)\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        # Predict function using Heaviside step function\n",
        "        Z = self.linear(inputs)\n",
        "        return self.Heaviside_step_fn(Z)\n",
        "\n",
        "    def loss(self, prediction, target):\n",
        "        # Calculate loss\n",
        "        return target - prediction\n",
        "\n",
        "    def train(self, inputs, target):\n",
        "        # Update weights based on prediction error\n",
        "        prediction = self.predict(inputs)\n",
        "        error = self.loss(prediction, target)\n",
        "        self.weights[1:] = self.weights[1:] + self.learning_rate * error * inputs\n",
        "        self.weights[0] = self.weights[0] + self.learning_rate * error\n",
        "\n",
        "    def fit(self, X, y, num_epochs):\n",
        "        # Fit the model to the data for a number of epochs\n",
        "        # 'num_epochs' = number of times we go through the entire training dataset\n",
        "        for epoch in range(num_epochs):\n",
        "            for inputs, target in zip(X, y):\n",
        "                self.train(inputs, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2587LvE2ZGr"
      },
      "source": [
        "Let's test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrh_i-4O2ZGr"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "# Training data:\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 1])\n",
        "\n",
        "# Initialize and train perceptron\n",
        "perceptron = OurPerceptron(num_inputs=2, learning_rate=0.1)\n",
        "perceptron.fit(X, y, num_epochs=10)\n",
        "\n",
        "# Test perceptron\n",
        "print(perceptron.predict([0, 0]))  # Output: 0\n",
        "print(perceptron.predict([0, 1]))  # Output: 1\n",
        "print(perceptron.predict([1, 0]))  # Output: 1\n",
        "print(perceptron.predict([1, 1]))  # Output: 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUAtZB2k2ZGr"
      },
      "source": [
        "Seems it works!üéâ Let's try something more interesting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMZN4mvv2ZGr"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a linearly separable dataset with two classes\n",
        "X, y = make_blobs(n_samples=1000, n_features=2, centers=2, cluster_std=1.8, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Initialize the Perceptron with the appropriate number of inputs\n",
        "perceptron = OurPerceptron(num_inputs = X_train.shape[1])\n",
        "\n",
        "# Train the Perceptron on the training data\n",
        "perceptron.fit(X_train, y_train, num_epochs=1000)\n",
        "\n",
        "# Prediction\n",
        "y_pred_our = perceptron.predict(X_test)\n",
        "\n",
        "# Test the accuracy of the trained Perceptron on the testing data\n",
        "accuracy = np.mean(y_pred_our != y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Plot the dataset\n",
        "plt.figure(figsize=(4,3))\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_our)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jnpa8r-2ZGr"
      },
      "outputs": [],
      "source": [
        "y_pred_our"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rSTus0p2ZGr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQY6deQm2ZGs"
      },
      "source": [
        "To technical? Don't worry, there's a built-in functionality in scikit-learn library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bzqme6R2ZGs"
      },
      "outputs": [],
      "source": [
        "# we have just the same X, y, and then also same X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnV-XPHH2ZGs"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# Create a perceptron classifier\n",
        "clf = Perceptron(eta0=0.1, max_iter=1000)  # eta0 is learning rate\n",
        "\n",
        "# Train the perceptron classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_sklearn = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = sklearn.metrics.accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvVHSPJf2ZGs"
      },
      "outputs": [],
      "source": [
        "y_pred_our - y_pred_sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJPoT6N52ZGu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(4,3))\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_sklearn)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et6pDEUu2ZGu"
      },
      "source": [
        "let's see both graphs together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7plN11FR2ZGu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,3), dpi=100)\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_our)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid()\n",
        "plt.title(\"our perceptron\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_sklearn)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid()\n",
        "plt.title(\"sklearn's perceptron\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9NizfBW2ZGu"
      },
      "source": [
        "The biggest difference is execution time üòâ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXGqNY-v2ZGu"
      },
      "outputs": [],
      "source": [
        "perceptron.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI7r5o142ZGu"
      },
      "source": [
        "Let's draw a line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M31F6Sf82ZGu"
      },
      "outputs": [],
      "source": [
        "# Extract weights\n",
        "bias = perceptron.weights[0]\n",
        "w1 = perceptron.weights[1]\n",
        "w2 = perceptron.weights[2]\n",
        "\n",
        "# Compute slope and intercept\n",
        "slope = -w1 / w2\n",
        "intercept = -bias / w2\n",
        "\n",
        "# Generate x values for plotting\n",
        "x_vals = np.linspace(np.min(X_test[:, 0]), np.max(X_test[:, 0]), 100)\n",
        "y_vals = slope * x_vals + intercept\n",
        "\n",
        "# Plot the scatter plot with decision boundary\n",
        "plt.figure(figsize=(4,3))\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_our)\n",
        "plt.plot(x_vals, y_vals, '-r', label='Decision Boundary')  # '-r' for red line\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8S9NtU2ZGv"
      },
      "source": [
        "Very useful&important way to check how well did you classify is **confusion matrix** (which can be either represented as a matrix or visualised as a figure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndwx9Zzo2ZGv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_our)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_matrix)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YViru4zW2ZGv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl8T6TcJ2ZGv"
      },
      "source": [
        "### Support Vector Machine (SVM)\n",
        " = algorithm of supervised ML, usually used for classification task\n",
        " [https://scikit-learn.org/stable/modules/svm.html#svm-classification](https://scikit-learn.org/stable/modules/svm.html#svm-classification)\n",
        "\n",
        " **The core idea:** to find a hyperplane that best divides a dataset into classes.\n",
        "\n",
        " A hyperplane is a decision boundary that separates different classes in the feature space. In two dimensions, this hyperplane is a line, but in higher dimensions, it can be a plane or a hyperplane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEF7Z1tC2ZGw"
      },
      "source": [
        "A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks.\n",
        "Intuitively, a good separation is achieved by the hyper-plane that **has the largest distance to the nearest training data points** of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n",
        "\n",
        "The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called ‚Äúsupport vectors‚Äù:\n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_001.png\" alt=\"banner\" width=\"450\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsjBKvWx2ZGw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate random data using make_blobs\n",
        "x, y = make_blobs(n_samples=300, centers=2, cluster_std=1.8, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit SVM classifier\n",
        "model = SVC(kernel='linear')   # {'precomputed', 'linear', 'poly', 'rbf', 'sigmoid'}\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = model.score(x_train, y_train)\n",
        "test_accuracy = model.score(x_test, y_test)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Testing Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muwEM9Ob2ZGw"
      },
      "outputs": [],
      "source": [
        "# #################################################\n",
        "# Plot decision boundary (works only for 2D features)\n",
        "plt.figure(figsize=(6,5), dpi=120)\n",
        "if x_train.shape[1] == 2:\n",
        "    # Create a meshgrid to plot decision boundary\n",
        "    h = 0.02  # Step size in the mesh\n",
        "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Plot decision boundary\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap=plt.cm.RdBu, edgecolors='k', label=\"Training data\")\n",
        "plt.scatter(x_test[:, 0], x_test[:, 1], c=y_test, cmap=plt.cm.RdBu, edgecolors='k', marker='x', label=\"Testing data\")\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.title('SVM Classification')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzvc4QTo2ZGw"
      },
      "source": [
        "üí° Perceptron classifier can be used only for *binary* classification. But it's not the case for SVM! Try to modify number of groups (`centers` in `make_blobs`) and see how the image above changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu2cAd412ZGx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WiQjHcY2ZGx"
      },
      "source": [
        "### k-means clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8-T2qq12ZGx"
      },
      "source": [
        " = algorithm for *clustering* data , i.e. splitting the data into groups based on their similarity. The number of clusters (groups) of given (either by the formulation of the task, or by your expert knowledge, or by computational limits, ...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9rPewkj2ZGx"
      },
      "source": [
        "**k-means algorithm**:\n",
        "1) choose a number of clusters, $k$.\n",
        "2) randomly select $k$ points rom your dataset as initial centroids (*centroid* = a point situated in a center of $k$-th cluster).\n",
        "3) assign each datapoint from your data to the nearest centroid. This is how you get $k$ clusters.\n",
        "4) **update step**: find a new centroid of each cluster by calculating the mean value of all data points assigned to this cluster\n",
        "5) repeat from step 3) until either centroids no longer change (convergence is reached) or a maximum number of iterations is reached (we don't want our algorithms to run forever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NhJ6ev62ZGy"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X, _ = make_blobs(n_samples=100, centers=2, cluster_std=1.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS6QK-go2ZGy"
      },
      "source": [
        "üö® Notice: it's unsupervised learning! We don't know \"the answers\", that's why we don't need `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ah3ysqL2ZGy"
      },
      "outputs": [],
      "source": [
        "# Model training\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(5,4), dpi=100) # figure is initialised\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_) # plot the datapoints (on x-axis 0th feature, on y-axis 1st feature, different colours acc. to kmeans.labels_)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red')  # plot centroids\n",
        "plt.title('K-Means Clustering')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntMdZR1j2ZGy"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnf9fjoJ2ZGy"
      },
      "source": [
        "## Irises - one more time:\n",
        "\n",
        "Let's try to look again on a simple dataset - iris dataset.\n",
        "\n",
        "Our task now will be:\n",
        "1) understand its properties (using different techniques we have already learned)\n",
        "2) try to build up a classifier, which can predict what class iris belongs to based on values of sepal/petal length/width\n",
        "\n",
        "Practical use-case: you pick a flower, you measure it -> you want a reliable software that will classify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQGLUlB42ZGy"
      },
      "outputs": [],
      "source": [
        "df_iris = sns.load_dataset('iris')\n",
        "\n",
        "df_iris.head(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTyhqOQT2ZG0"
      },
      "outputs": [],
      "source": [
        "df_iris.describe(include='all')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJIBl4fb2ZG0"
      },
      "source": [
        "Dataset contains 150 observations of iris flowers. The parameters measured were lengths and widths, the \"response\" variable was the only categorical one present in this dataset: the name of species of the observed flower (3 possible values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFmURca92ZG0"
      },
      "outputs": [],
      "source": [
        "df_iris[\"species\"] = df_iris[\"species\"].astype(\"category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl7OxNd-2ZG0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(3,2), dpi=120)\n",
        "sns.histplot(data=df_iris['sepal_length'])\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSFhYlu-2ZG0"
      },
      "source": [
        "üìà 3d picture of the dataset --> separate script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxz8HU3b2ZG0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,3), dpi=120)\n",
        "plt.subplot(1,3,1)\n",
        "sns.boxplot(x='species', y='sepal_length', data=df_iris, hue='species')\n",
        "plt.title(\"sepal length\")\n",
        "plt.subplot(1,3,2)\n",
        "sns.boxplot(x='species', y='sepal_width', data=df_iris, hue='species')\n",
        "plt.title(\"sepal width\")\n",
        "plt.subplot(1,3,3)\n",
        "sns.boxplot(x='species', y='petal_length', data=df_iris, hue='species')\n",
        "plt.title(\"petal length\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdF0WgLh2ZG1"
      },
      "source": [
        "Pairplot makes the first impression of dataset for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIqOfWb02ZG1"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data=df_iris, hue='species')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6jZq6W52ZG1"
      },
      "source": [
        "Let's also visualise a statistical relationship:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2invTI-52ZG1"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x=\"sepal_length\", y=\"petal_length\", data=df_iris, hue = 'species')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcrGvceP2ZG1"
      },
      "source": [
        "We can suspect that classes can be seperated. Let's use SVM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pbxl0sT2ZG2"
      },
      "outputs": [],
      "source": [
        "df_iris['species'] = pd.factorize(df_iris['species'])[0] + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ7uWW5V2ZG2"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = df_iris.drop('species', axis=1)\n",
        "y = df_iris['species']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create LinearSVC classifier\n",
        "clf = LinearSVC()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35SdBgPU2ZG2"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD8rInbv2ZG2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,3))\n",
        "plt.subplot(1,3,1)\n",
        "plt.scatter(X_test['sepal_length'], X_test['sepal_width'], c=y_pred)\n",
        "plt.xlabel('sepal_length')\n",
        "plt.ylabel('sepal_width')\n",
        "plt.grid()\n",
        "plt.subplot(1,3,2)\n",
        "plt.scatter(X_test['petal_length'], X_test['petal_width'], c=y_pred)\n",
        "plt.xlabel('petal_length')\n",
        "plt.ylabel('petal_width')\n",
        "plt.grid()\n",
        "plt.subplot(1,3,3)\n",
        "plt.scatter(X_test['sepal_length'], X_test['petal_length'], c=y_pred)\n",
        "plt.xlabel('sepal_length')\n",
        "plt.ylabel('petal_length')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4eH2CLR2ZG2"
      },
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = conf_matrix)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybGav-tB2ZG2"
      },
      "source": [
        "Sometimes it's better to have a 'curvy' decision border. For that purpose, you can use several modifications of SVM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeGbLhOa2ZG2"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm, datasets\n",
        "\n",
        "def make_meshgrid(x, y, h=.02):\n",
        "    \"\"\"Create a mesh of points to plot in\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: data to base x-axis meshgrid on\n",
        "    y: data to base y-axis meshgrid on\n",
        "    h: stepsize for meshgrid, optional\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    xx, yy : ndarray\n",
        "    \"\"\"\n",
        "    x_min, x_max = x.min() - 1, x.max() + 1\n",
        "    y_min, y_max = y.min() - 1, y.max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    return xx, yy\n",
        "\n",
        "\n",
        "def plot_contours(ax, clf, xx, yy, **params):\n",
        "    \"\"\"Plot the decision boundaries for a classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ax: matplotlib axes object\n",
        "    clf: a classifier\n",
        "    xx: meshgrid ndarray\n",
        "    yy: meshgrid ndarray\n",
        "    params: dictionary of params to pass to contourf, optional\n",
        "    \"\"\"\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    out = ax.contourf(xx, yy, Z, **params)\n",
        "    return out\n",
        "\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "# Take the first two features.\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# we create an instance of SVM and fit out data. We do not scale our\n",
        "# data since we want to plot the support vectors\n",
        "C = 1.0  # SVM regularization parameter\n",
        "models = (svm.SVC(kernel='linear', C=C),\n",
        "          svm.LinearSVC(C=C, max_iter=10000),\n",
        "          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
        "          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))\n",
        "models = (clf.fit(X, y) for clf in models)\n",
        "\n",
        "# title for the plots\n",
        "titles = ('SVC with linear kernel',\n",
        "          'LinearSVC (linear kernel)',\n",
        "          'SVC with RBF kernel',\n",
        "          'SVC with polynomial (degree 3) kernel')\n",
        "\n",
        "# Set-up 2x2 grid for plotting.\n",
        "fig, sub = plt.subplots(2, 2,figsize=(12,8))\n",
        "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "\n",
        "X0, X1 = X[:, 0], X[:, 1]\n",
        "xx, yy = make_meshgrid(X0, X1)\n",
        "\n",
        "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
        "    plot_contours(ax, clf, xx, yy,\n",
        "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    ax.set_xlabel('Sepal length')\n",
        "    ax.set_ylabel('Sepal width')\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.set_title(title)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWDO6JL6r14R"
      },
      "source": [
        "## Concluding remarks and advices\n",
        " - remember [Occam's razor]() (aka principle of parsimony, aka [KISS principle](https://en.wikipedia.org/wiki/KISS_principle)):\n",
        "\n",
        "        > \"The simplest explanation is usually the best one.\"\n",
        " -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSP8oxmBr77B"
      },
      "source": [
        "## Additional sources (where to seek for information):\n",
        " - Intro to ML made by Google (free to use) [here](https://developers.google.com/machine-learning/intro-to-ml)\n",
        " - tutorial on ML on w3schools: [here](https://www.w3schools.com/python/python_ml_getting_started.asp)\n",
        " - lectures from Snaford University [here](https://ee104.stanford.edu/lectures/overview.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbOrvLdd2ZG3"
      },
      "source": [
        "## Where to learn more:\n",
        "\n",
        " - Course on ML at Stanford, by **Andrew Ng** [https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)\n",
        " - Intro to ML made by Google (very simple, free to use) [https://developers.google.com/machine-learning/intro-to-ml/what-is-ml](https://developers.google.com/machine-learning/intro-to-ml/what-is-ml)\n",
        " - simple webpage about ML (from Stanford) [https://stanford.edu/~shervine/teaching/cs-229/](https://stanford.edu/~shervine/teaching/cs-229/)\n",
        " - tutorial on ML on w3schools: [here](https://www.w3schools.com/python/python_ml_getting_started.asp)\n",
        " - lectures from Snaford University [here](https://ee104.stanford.edu/lectures/overview.pdf)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfNKEljPeUGs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpMOBCrX2ZG3"
      },
      "source": [
        "========================================================================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSSxoGZ32ZG3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install art\n",
        "import art"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exvthMbU2ZG4"
      },
      "outputs": [],
      "source": [
        "art.tprint(\"Thank you \\n for your attention!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVHh5pkZ2ZG4"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "generative_ai_disabled": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}